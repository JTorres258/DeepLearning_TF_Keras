{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks in Tensorflow I\n",
    "\n",
    "### from:\n",
    "### http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <font color='green'>Outline of the data</font>\n",
    "\n",
    "In this post, we’ll be building a no frills RNN that accepts a binary sequence X and uses it to predict a binary sequence Y. The sequences are constructed as follows:\n",
    "\n",
    "    Input sequence (X): At time step t, Xt has a 50% chance of being 1 (and a 50% chance of being 0). E.g., X might be [1, 0, 0, 1, 1, 1 … ].\n",
    "    \n",
    "    Output sequence (Y): At time step t, Yt has a base 50% chance of being 1 (and a 50% base chance to be 0). The \n",
    "    chance of Yt being 1 is increased by 50% (i.e., to 100%) if Xt−3 is 1, and decreased by 25% (i.e., to 25%) if Xt−8 is 1. If both Xt−3 and Xt−8 are 1, the chance of Yt being 1 is 50% + 50% - 25% = 75%.\n",
    "\n",
    "Thus, there are two dependencies in the data: one at t-3 (3 steps back) and one at t-8 (8 steps back)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected cross-entropy\n",
    "\n",
    "This data is simple enough that we can calculate the expected cross-entropy loss for a trained RNN depending on whether or not it learns the dependencies:\n",
    "\n",
    "* If the network learns no dependencies, it will correctly assign a probability of 62.5% to 1, for an expected cross-entropy loss of about 0.66.\n",
    "* If the network learns only the first dependency (3 steps back) but not the second dependency, it will correctly assign a probability of 87.5%, 50% of the time, and correctly assign a probability of 62.5% the other 50% of the time, for an expected cross entropy loss of about 0.52.\n",
    "* If the network learns both dependencies, it will be 100% accurate 25% of the time, correctly assign a probability of 50%, 25% of the time, and correctly assign a probability of 75%, 50% of the time, for an expected cross extropy loss of about 0.45.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected cross entropy loss if the model:\n",
      "- learns neither dependency: 0.661563238158\n",
      "- learns first dependency:   0.519166699707\n",
      "- learns both dependencies:  0.454454367449\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Expected cross entropy loss if the model:\")\n",
    "print(\"- learns neither dependency:\", -(0.625 * np.log(0.625) +\n",
    "                                      0.375 * np.log(0.375)))\n",
    "# Learns first dependency only ==> 0.51916669970720941\n",
    "print(\"- learns first dependency:  \",\n",
    "      -0.5 * (0.875 * np.log(0.875) + 0.125 * np.log(0.125))\n",
    "      -0.5 * (0.625 * np.log(0.625) + 0.375 * np.log(0.375)))\n",
    "print(\"- learns both dependencies: \", -0.50 * (0.75 * np.log(0.75) + 0.25 * np.log(0.25))\n",
    "      - 0.25 * (2 * 0.50 * np.log (0.50)) - 0.25 * (0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>First: dealing with data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Generate our binary sequences**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Prepare for feeding data into the graph:** *from raw data to batches*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted from \n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Second: we will start with a vanilla RNN model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://r2rt.com/static/images/BasicRNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...and we will use static tf.nn.rnn so it uses lists of tensors to represent the width = num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The easiest way to represent each type of duplicate tensor (the rnn inputs, the rnn outputs (hidden state), the predictions, and the loss) is as a list of tensors\n",
    "\n",
    "<img src=\"http://r2rt.com/static/images/BasicRNNLabeled.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Set GLOBAL Configuration Variables**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_epochs=1\n",
    "num_steps = 5 # number of truncated backprop steps\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 8\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Defining the graph model**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "RNN Inputs : modifications due to one-ho-encoding problems:\n",
    "\n",
    "  1  !!! TRANSPOSE FOR THIS!!! nn_inputs is a \n",
    "      list of num_steps tensors with shape [batch_size, num_classe\n",
    "  2.,. MUST be int64 !!!\n",
    "  3.- add 1,0 (ONE HOT Using  0 ... 1)\n",
    "\"\"\"\n",
    "\n",
    "# Turn our x placeholder into a list of one-hot tensors:\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "# NEED tocast to int64\n",
    "# ORIGINAL x_one_hot = tf.one_hot(x, num_classes)\n",
    "\n",
    "\n",
    "x_one_hot = tf.one_hot(tf.cast(tf.transpose(x, perm=[1, 0]), tf.int64), num_classes, 1,0)\n",
    "\n",
    "#rnn_inputs = tf.unpack(tf.cast(x_one_hot, tf.float32))\n",
    "#TF 1.1 change unpack for unstack\n",
    "rnn_inputs = tf.unstack(tf.cast(x_one_hot, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Definition of a BasicRNNCell**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definition of rnn_cell\n",
    "\n",
    "This is very similar to the __call__ method on Tensorflow's BasicRNNCell. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py\n",
    "\"\"\"\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "        # sintax in TF1... !!\n",
    "        return tf.tanh(tf.matmul(tf.concat([rnn_input, state],1), W) + b)\n",
    "        #return tf.tanh(tf.matmul(tf.concat(1, [rnn_input, state]), W) + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Adding run_cells to graph**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adding rnn_cells to graph\n",
    "\n",
    "This is a simplified version of the \"rnn\" function from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py\n",
    "\"\"\"\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Output y(t) (softmax) Logits and Predictions**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\n",
    "Losses and total_loss are simlar to the \"sequence_loss_by_example\" and \"sequence_loss\"\n",
    "functions, respectively, from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py\n",
    "\"\"\"\n",
    "\n",
    "#logits and predictions\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Loss function and training step**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turn our y placeholder into a list labels\n",
    "\n",
    "# New sintax y TF 1...\n",
    "y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(y,num_steps,1)]\n",
    "\n",
    "#y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, num_steps, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#losses and train_step\n",
    "\n",
    "# adding logits= ... for TF 1..\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit,labels=label) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "losses_last=losses[-1]\n",
    "total_loss = tf.reduce_mean(losses[-1])\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Third: train the network</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Function to train the network**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to train the network\n",
    "\"\"\"\n",
    "\n",
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        # TF1 ..\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        print(\"Starting for idx...\")\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses_last,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 250 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Train the network**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting for idx...\n",
      "\n",
      "EPOCH 0\n",
      "Average loss at step 100 for last 250 steps: 0.53339717716\n",
      "Average loss at step 200 for last 250 steps: 0.496033110321\n",
      "Average loss at step 300 for last 250 steps: 0.493903718889\n",
      "Average loss at step 400 for last 250 steps: 0.482006113231\n",
      "Average loss at step 500 for last 250 steps: 0.486720716059\n",
      "Average loss at step 600 for last 250 steps: 0.489448575377\n",
      "Average loss at step 700 for last 250 steps: 0.488727093637\n",
      "Average loss at step 800 for last 250 steps: 0.484382275939\n",
      "Average loss at step 900 for last 250 steps: 0.481926053166\n"
     ]
    }
   ],
   "source": [
    "training_losses = train_network(num_epochs,num_steps,state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Plotting training losses**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7df7bb3e80>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7df792ee48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Finally: improve the model playing with hyperparameters num_steps state_size</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**And try to understand your results!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## then...see next Notebook ...translating our model to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
