{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks in Tensorflow I\n",
    "\n",
    "### from:\n",
    "### http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <font color='green'>Outline of the data</font>\n",
    "\n",
    "In this post, we’ll be building a no frills RNN that accepts a binary sequence X and uses it to predict a binary sequence Y. The sequences are constructed as follows:\n",
    "\n",
    "    Input sequence (X): At time step t, Xt has a 50% chance of being 1 (and a 50% chance of being 0). E.g., X might be [1, 0, 0, 1, 1, 1 … ].\n",
    "    \n",
    "    Output sequence (Y): At time step t, Yt has a base 50% chance of being 1 (and a 50% base chance to be 0). The \n",
    "    chance of Yt being 1 is increased by 50% (i.e., to 100%) if Xt−3 is 1, and decreased by 25% (i.e., to 25%) if Xt−8 is 1. If both Xt−3 and Xt−8 are 1, the chance of Yt being 1 is 50% + 50% - 25% = 75%.\n",
    "\n",
    "Thus, there are two dependencies in the data: one at t-3 (3 steps back) and one at t-8 (8 steps back)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected cross-entropy\n",
    "\n",
    "This data is simple enough that we can calculate the expected cross-entropy loss for a trained RNN depending on whether or not it learns the dependencies:\n",
    "\n",
    "* If the network learns no dependencies, it will correctly assign a probability of 62.5% to 1, for an expected cross-entropy loss of about 0.66.\n",
    "* If the network learns only the first dependency (3 steps back) but not the second dependency, it will correctly assign a probability of 87.5%, 50% of the time, and correctly assign a probability of 62.5% the other 50% of the time, for an expected cross entropy loss of about 0.52.\n",
    "* If the network learns both dependencies, it will be 100% accurate 25% of the time, correctly assign a probability of 50%, 25% of the time, and correctly assign a probability of 75%, 50% of the time, for an expected cross extropy loss of about 0.45.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected cross entropy loss if the model:\n",
      "('- learns neither dependency:', 0.66156323815798213)\n",
      "('- learns first dependency:  ', 0.51916669970720941)\n",
      "('- learns both dependencies: ', 0.4544543674493905)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Expected cross entropy loss if the model:\")\n",
    "print(\"- learns neither dependency:\", -(0.625 * np.log(0.625) +\n",
    "                                      0.375 * np.log(0.375)))\n",
    "# Learns first dependency only ==> 0.51916669970720941\n",
    "print(\"- learns first dependency:  \",\n",
    "      -0.5 * (0.875 * np.log(0.875) + 0.125 * np.log(0.125))\n",
    "      -0.5 * (0.625 * np.log(0.625) + 0.375 * np.log(0.375)))\n",
    "print(\"- learns both dependencies: \", -0.50 * (0.75 * np.log(0.75) + 0.25 * np.log(0.25))\n",
    "      - 0.25 * (2 * 0.50 * np.log (0.50)) - 0.25 * (0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>First: dealing with data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Generate our binary sequences**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Prepare for feeding data into the graph:** *from raw data to batches*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted from \n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Second: we will start with a vanilla RNN model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://r2rt.com/static/images/BasicRNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...and we will use static tf.nn.rnn so it uses lists of tensors to represent the width = num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The easiest way to represent each type of duplicate tensor (the rnn inputs, the rnn outputs (hidden state), the predictions, and the loss) is as a list of tensors\n",
    "\n",
    "<img src=\"http://r2rt.com/static/images/BasicRNNLabeled.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Set GLOBAL Configuration Variables**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_epochs=1\n",
    "num_steps = 5 # number of truncated backprop steps\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 8\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Defining the graph model**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "RNN Inputs : modifications due to one-ho-encoding problems:\n",
    "\n",
    "  1  !!! TRANSPOSE FOR THIS!!! nn_inputs is a \n",
    "      list of num_steps tensors with shape [batch_size, num_classe\n",
    "  2.,. MUST be int64 !!!\n",
    "  3.- add 1,0 (ONE HOT Using  0 ... 1)\n",
    "\"\"\"\n",
    "\n",
    "# Turn our x placeholder into a list of one-hot tensors:\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "# NEED tocast to int64\n",
    "# ORIGINAL x_one_hot = tf.one_hot(x, num_classes)\n",
    "\n",
    "\n",
    "x_one_hot = tf.one_hot(tf.cast(tf.transpose(x, perm=[1, 0]), tf.int64), num_classes, 1,0)\n",
    "\n",
    "rnn_inputs = tf.unpack(tf.cast(x_one_hot, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Definition of a BasicRNNCell**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definition of rnn_cell\n",
    "\n",
    "This is very similar to the __call__ method on Tensorflow's BasicRNNCell. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py\n",
    "\"\"\"\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat(1, [rnn_input, state]), W) + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Adding run_cells to graph**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adding rnn_cells to graph\n",
    "\n",
    "This is a simplified version of the \"rnn\" function from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py\n",
    "\"\"\"\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Output y(t) (softmax) Logits and Predictions**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\n",
    "Losses and total_loss are simlar to the \"sequence_loss_by_example\" and \"sequence_loss\"\n",
    "functions, respectively, from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py\n",
    "\"\"\"\n",
    "\n",
    "#logits and predictions\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Loss function and training step**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turn our y placeholder into a list labels\n",
    "y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, num_steps, y)]\n",
    "\n",
    "#losses and train_step\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logit,label) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "losses_last=losses[-1]\n",
    "total_loss = tf.reduce_mean(losses[-1])\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Third: train the network</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Function to train the network**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to train the network\n",
    "\"\"\"\n",
    "\n",
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        print(\"Starting for idx...\")\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses_last,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 250 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Train the network**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting for idx...\n",
      "('\\nEPOCH', 0)\n",
      "('Average loss at step', 100, 'for last 250 steps:', 0.59573681414127355)\n",
      "('Average loss at step', 200, 'for last 250 steps:', 0.49620730370283128)\n",
      "('Average loss at step', 300, 'for last 250 steps:', 0.48795114874839784)\n",
      "('Average loss at step', 400, 'for last 250 steps:', 0.4861856997013092)\n",
      "('Average loss at step', 500, 'for last 250 steps:', 0.48234885543584821)\n",
      "('Average loss at step', 600, 'for last 250 steps:', 0.49293043345212939)\n",
      "('Average loss at step', 700, 'for last 250 steps:', 0.47967623233795165)\n",
      "('Average loss at step', 800, 'for last 250 steps:', 0.4848884266614914)\n",
      "('Average loss at step', 900, 'for last 250 steps:', 0.48608504503965377)\n"
     ]
    }
   ],
   "source": [
    "training_losses = train_network(num_epochs,num_steps,state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Plotting training losses**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f39c6e51810>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0hgffgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3a10fa9710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Finally: improve the model playing with hyperparameters num_steps state_size</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**And try to understand your results!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## then...see next Notebook ...translating our model to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}