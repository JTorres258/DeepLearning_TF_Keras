{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Example from: Recurrent Neural Networks in Tensorflow II\n",
    "\n",
    "http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## <font color='purple'>We will use LSTM and GRU </font>\n",
    "* ## <font color='purple'>Simple NLP Task: character-level language model to generate character sequences </font>\n",
    "### a la Andrej Karpathy’s char-rnn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3'> We’ll use the tiny-shakespeare corpus as our data, though we could use any plain text file.\n",
    "* We’ll choose to use all of the characters in the text file as our vocabulary, treating lowercase and capital letters are separate characters. </font>\n",
    "\n",
    "Additionally, it is likely a good idea to restrict the vocabulary (i.e., the set of characters) used, by replacing uncommon characters with an UNK token (like a square: □)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "# No in TF 1.\n",
    "#from tensorflow.models.rnn.ptb import reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 1.1.0\n",
      "Summary: TensorFlow helps the tensors flow\n",
      "Home-page: http://tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: opensource@google.com\n",
      "License: Apache 2.0\n",
      "Location: c:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\n",
      "Requires: protobuf, wheel, six, werkzeug, numpy\n"
     ]
    }
   ],
   "source": [
    "! pip show tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='brown'>Update tensorflow to 0.10.0 for managing state_is_tuple=True in LSTM (see below) </font>\n",
    "\n",
    "* ### cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "* ### cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>The task:  generate character sequences</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load and data\n",
    "\"\"\"\n",
    "\n",
    "file_url = 'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'\n",
    "file_name = 'tinyshakespeare.txt'\n",
    "\n",
    "#file_url = 'http://latel.upf.edu/traductica/scp/quijote/quijote.txt'\n",
    "#file_name = 'cervantes.txt'\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(file_url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open(file_name,'r') as f:\n",
    "    raw_data = f.read()\n",
    "    print(\"Data length:\", len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print(raw_data[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## understand : vocab : unique elements in raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(raw_data)\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q', '.', 'K', '-', 'O', '3', 'm', 'U', 'H', 'e', ':', ' ', 'B', 'R', 'X', 'Y', 's', 'S', 'c', 'i', '$', 'Z', 'T', 'b', 'g', \"'\", 'f', 'l', 'M', 'L', 'v', 'E', 'w', ',', 'P', 'z', 'W', 'N', 't', '\\n', ';', 'F', 'C', 'p', 'k', 'd', 'I', 'q', 'y', 'r', '!', 'a', 'J', 'x', '?', 'h', 'j', 'G', 'u', 'n', 'V', 'o', 'D', 'A', '&'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Q',\n",
       " 1: '.',\n",
       " 2: 'K',\n",
       " 3: '-',\n",
       " 4: 'O',\n",
       " 5: '3',\n",
       " 6: 'm',\n",
       " 7: 'U',\n",
       " 8: 'H',\n",
       " 9: 'e',\n",
       " 10: ':',\n",
       " 11: ' ',\n",
       " 12: 'B',\n",
       " 13: 'R',\n",
       " 14: 'X',\n",
       " 15: 'Y',\n",
       " 16: 's',\n",
       " 17: 'S',\n",
       " 18: 'c',\n",
       " 19: 'i',\n",
       " 20: '$',\n",
       " 21: 'Z',\n",
       " 22: 'T',\n",
       " 23: 'b',\n",
       " 24: 'g',\n",
       " 25: \"'\",\n",
       " 26: 'f',\n",
       " 27: 'l',\n",
       " 28: 'M',\n",
       " 29: 'L',\n",
       " 30: 'v',\n",
       " 31: 'E',\n",
       " 32: 'w',\n",
       " 33: ',',\n",
       " 34: 'P',\n",
       " 35: 'z',\n",
       " 36: 'W',\n",
       " 37: 'N',\n",
       " 38: 't',\n",
       " 39: '\\n',\n",
       " 40: ';',\n",
       " 41: 'F',\n",
       " 42: 'C',\n",
       " 43: 'p',\n",
       " 44: 'k',\n",
       " 45: 'd',\n",
       " 46: 'I',\n",
       " 47: 'q',\n",
       " 48: 'y',\n",
       " 49: 'r',\n",
       " 50: '!',\n",
       " 51: 'a',\n",
       " 52: 'J',\n",
       " 53: 'x',\n",
       " 54: '?',\n",
       " 55: 'h',\n",
       " 56: 'j',\n",
       " 57: 'G',\n",
       " 58: 'u',\n",
       " 59: 'n',\n",
       " 60: 'V',\n",
       " 61: 'o',\n",
       " 62: 'D',\n",
       " 63: 'A',\n",
       " 64: '&'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_idx['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_vocab[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 39,\n",
       " ' ': 11,\n",
       " '!': 50,\n",
       " '$': 20,\n",
       " '&': 64,\n",
       " \"'\": 25,\n",
       " ',': 33,\n",
       " '-': 3,\n",
       " '.': 1,\n",
       " '3': 5,\n",
       " ':': 10,\n",
       " ';': 40,\n",
       " '?': 54,\n",
       " 'A': 63,\n",
       " 'B': 12,\n",
       " 'C': 42,\n",
       " 'D': 62,\n",
       " 'E': 31,\n",
       " 'F': 41,\n",
       " 'G': 57,\n",
       " 'H': 8,\n",
       " 'I': 46,\n",
       " 'J': 52,\n",
       " 'K': 2,\n",
       " 'L': 29,\n",
       " 'M': 28,\n",
       " 'N': 37,\n",
       " 'O': 4,\n",
       " 'P': 34,\n",
       " 'Q': 0,\n",
       " 'R': 13,\n",
       " 'S': 17,\n",
       " 'T': 22,\n",
       " 'U': 7,\n",
       " 'V': 60,\n",
       " 'W': 36,\n",
       " 'X': 14,\n",
       " 'Y': 15,\n",
       " 'Z': 21,\n",
       " 'a': 51,\n",
       " 'b': 23,\n",
       " 'c': 18,\n",
       " 'd': 45,\n",
       " 'e': 9,\n",
       " 'f': 26,\n",
       " 'g': 24,\n",
       " 'h': 55,\n",
       " 'i': 19,\n",
       " 'j': 56,\n",
       " 'k': 44,\n",
       " 'l': 27,\n",
       " 'm': 6,\n",
       " 'n': 59,\n",
       " 'o': 61,\n",
       " 'p': 43,\n",
       " 'q': 47,\n",
       " 'r': 49,\n",
       " 's': 16,\n",
       " 't': 38,\n",
       " 'u': 58,\n",
       " 'v': 30,\n",
       " 'w': 32,\n",
       " 'x': 53,\n",
       " 'y': 48,\n",
       " 'z': 35}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_idx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### understand: converting text data into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [vocab_to_idx[c] for c in raw_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citi'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41, 19, 49, 16, 38, 11, 42, 19, 38, 19]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recover_data = [idx_to_vocab[c] for c in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recover_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del recover_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='brown'> Some utility functions for feeding batches</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ptb_iterator(raw_data, batch_size, num_steps):\n",
    "  \"\"\"Iterate on the raw PTB data.\n",
    "  This generates batch_size pointers into the raw PTB data, and allows\n",
    "  minibatch iteration along these pointers.\n",
    "  Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "  Yields:\n",
    "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
    "    The second element of the tuple is the same data time-shifted to the\n",
    "    right by one.\n",
    "  Raises:\n",
    "    ValueError: if batch_size or num_steps are too high.\n",
    "  \"\"\"\n",
    "  raw_data = np.array(raw_data, dtype=np.int32)\n",
    "\n",
    "  data_len = len(raw_data)\n",
    "  batch_len = data_len // batch_size\n",
    "  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "  for i in range(batch_size):\n",
    "    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
    "\n",
    "  epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "  if epoch_size == 0:\n",
    "    raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "  for i in range(epoch_size):\n",
    "    x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "    yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield ptb_iterator(data, batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  PTB from Penn Tree Bank (PTB) dataset\n",
    "\n",
    "<font color='green'>reader.ptb_iterator(data, batch_size, num_steps)<7font>\n",
    "\n",
    "https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "\n",
    "\n",
    "def ptb_iterator(raw_data, batch_size, num_steps):\n",
    "  \n",
    "  Iterate on the raw PTB data.\n",
    "  This generates batch_size pointers into the raw PTB data, and allows\n",
    "  minibatch iteration along these pointers.\n",
    "  \n",
    "  Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "    \n",
    "  Yields:\n",
    "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
    "    \n",
    "    The second element of the tuple is the same data time-shifted to the\n",
    "    right by one.\n",
    "    \n",
    "  Raises:\n",
    "    ValueError: if batch_size or num_steps are too high.\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no. = 0\n",
      "Total no. steps= 173\n",
      "X information....\n",
      "(32, 200)\n",
      "Y information....\n",
      "(32, 200)\n"
     ]
    }
   ],
   "source": [
    "num_epochs=100\n",
    "num_steps=200\n",
    "batch_size=32\n",
    "\n",
    "Xepoch=gen_epochs(num_epochs, num_steps, batch_size)\n",
    "\n",
    "for idx, epoch in enumerate(Xepoch):\n",
    "    print('epoch no. =',idx)\n",
    "    for step, (X, Y) in enumerate(epoch):\n",
    "            cc=0\n",
    "    \n",
    "    print(\"Total no. steps=\",step)\n",
    "    print(\"X information....\")\n",
    "    print(X.shape)\n",
    "    print(\"Y information....\")\n",
    "    print(Y.shape)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1107200"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "173*200*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174.2803125"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1115394/(32*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no. = 0\n",
      "0\n",
      "X information....\n",
      "(32, 200)\n",
      "<class 'numpy.ndarray'>\n",
      "[[41 19 49 ..., 48 61 58]\n",
      " [26 61 49 ...,  6 51 48]\n",
      " [11 42 61 ..., 41 19 49]\n",
      " ..., \n",
      " [61 58 11 ..., 58 16 43]\n",
      " [11 55 61 ..., 26 11 32]\n",
      " [11 26 61 ..., 61 16  9]]\n",
      "Y information....\n",
      "(32, 200)\n",
      "<class 'numpy.ndarray'>\n",
      "[[19 49 16 ..., 61 58 11]\n",
      " [61 49 11 ..., 51 48 11]\n",
      " [42 61 49 ..., 19 49 16]\n",
      " ..., \n",
      " [58 11 32 ..., 16 43 19]\n",
      " [55 61 32 ..., 11 32 61]\n",
      " [26 61 61 ..., 16  9 11]]\n"
     ]
    }
   ],
   "source": [
    "num_epochs=1\n",
    "num_steps=200\n",
    "batch_size=32\n",
    "\n",
    "Xepoch=gen_epochs(num_epochs, num_steps, batch_size)\n",
    "\n",
    "for idx, epoch in enumerate(Xepoch):\n",
    "    print('epoch no. =',idx)\n",
    "    for step, (X, Y) in enumerate(epoch):\n",
    "        if step % 500 == 0:\n",
    "            print(step)\n",
    "            print (\"X information....\")\n",
    "            print(X.shape)\n",
    "            print(type(X))\n",
    "            print(X[0:10])\n",
    "            print (\"Y information....\")\n",
    "            print(Y.shape)\n",
    "            print(type(Y))\n",
    "            print(Y[0:10])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for graph reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Second: RNN graph definition</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_size = 100\n",
    "num_classes = vocab_size\n",
    "batch_size = 32\n",
    "num_steps = 200\n",
    "num_layers = 3\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## <font color='red'>Create EMBEDDINGS</font>\n",
    "http://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "# Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#LSTM\n",
    "# for TF 1....\n",
    "#cell = tf.contrib.rnn.LSTMCell(state_size, state_is_tuple=True)\n",
    "#cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "\n",
    "#cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "#cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "\n",
    "# GRU\n",
    "# TF 1....\n",
    "cell = tf.contrib.rnn.GRUCell(state_size)\n",
    "#cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "\n",
    "\n",
    "\n",
    "init_state = cell.zero_state(batch_size, tf.float32)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "#reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Third: train RNN (LSTM or GRU)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for Epoch 0 : 3.70520788226\n",
      "It took 45.62564277648926 seconds this training.\n"
     ]
    }
   ],
   "source": [
    "num_epochs=1\n",
    "verbose=True\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "tf.set_random_seed(2345)\n",
    "with tf.Session() as sess:\n",
    "        #TF 1.\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            training_loss = 0\n",
    "            steps = 0\n",
    "            training_state = None\n",
    "            for X, Y in epoch:\n",
    "                steps += 1\n",
    "\n",
    "#                if training_state is not None:\n",
    "#                    feed_dict[g['init_state']] = training_state\n",
    " \n",
    "                training_loss_, training_state, _ = sess.run([total_loss,\n",
    "                                                      final_state,\n",
    "                                                      train_step],\n",
    "                                                          feed_dict={x: X, y: Y})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\"for last 100 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/steps)\n",
    "                    training_loss = 0\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", idx, \":\", training_loss/steps)\n",
    "            \n",
    "        saver = tf.train.Saver()\n",
    "        #saver.save(sess, 'RNN_GRU_model_cervantes')\n",
    "        saver.save(sess, '.\\RNN_GRU_model_shakespeare')\n",
    "            \n",
    "print(\"It took\", time.time() - t, \"seconds this training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some results:\n",
    "\n",
    "### LSTM\n",
    "* cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "* cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "+ ('Average training loss for Epoch', 0, ':', 3.5619977603013488)\n",
    "+ ('It took', 38.50640511512756, 'seconds this training.')\n",
    "\n",
    "### GRU\n",
    "* ('Average training loss for Epoch', 0, ':', 3.6105946466840546)\n",
    "* ('It took', 35.258342027664185, 'seconds this training.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working directory to save our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\MSTC_TFCourse\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'checkpoint', 'conv1.npz', 'data_with_labels.npz', 'MSTC_FontReco_CNN_TF1Py3.ipynb', 'MSTC_FontReco_FeedForward_TF1Py3.ipynb', 'MSTC_FontReco_LogisticReg_TF1Py3.ipynb', 'MSTC_IntroTF_1.ipynb', 'MSTC_IntroTF_2.ipynb', 'MSTC_RNN_1.ipynb', 'MSTC_RNN_2_dynamic_TF1Py3.ipynb', 'MSTC_RNN_3.ipynb', 'RNN_GRU_model_shakespeare.data-00000-of-00001', 'RNN_GRU_model_shakespeare.index', 'RNN_GRU_model_shakespeare.meta', 'temp.npz', 'TF_Upgrade11.ipynb', 'tinyshakespeare.txt']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove dir not empty + sub dirs\n",
    "#import shutil\n",
    "\n",
    "#shutil.rmtree('./Ubi_Voice.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Finally: models (LSTM or GRU) can be used to generate TEXT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## First: <font size='3'>we need to rebuild the graph so as to accept a single character at a time</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_size = 100\n",
    "num_classes = vocab_size\n",
    "batch_size = 1\n",
    "num_steps = 1\n",
    "num_layers = 3\n",
    "learning_rate = 1e-4\n",
    "num_epochs=1\n",
    "\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "# Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "    \n",
    "    \n",
    "#LSTM\n",
    "# for TF 1....\n",
    "#cell = tf.contrib.rnn.LSTMCell(state_size, state_is_tuple=True)\n",
    "#cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "\n",
    "#cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "#cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "\n",
    "# GRU\n",
    "# TF 1....\n",
    "cell = tf.contrib.rnn.GRUCell(state_size)\n",
    "#cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "\n",
    "\n",
    "init_state = cell.zero_state(batch_size, tf.float32)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "#reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then:\n",
    "* <font size='3'>Restore our saved model.\n",
    "* We’ll give the network a single character prompt,i.e. prompt='A'\n",
    "* Grab its predicted probability distribution for the next character\n",
    "* Use that distribution to pick the next character, and repeat. </font>\n",
    "###   \n",
    "<font size='3'>When picking the next character, using pick_top_chars != None to use the whole probability distribution (default), or be forced to pick one of the top n most likely characters in the distribution. The latter option should obtain more English-like results.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from RNN_GRU_model_shakespeare\n",
      "A h heh  h heet e  eo toe  h a  ataeo  htat  aeae eaet e  h a h t hoa o t eoha  aaoe t eeoto e eeateae   h h  aa   ee to t toee o e o eoe    at haat t e h   ht ha h  a ettoeo tt eo  ha tte  t ett te  a ett  he eaa   e h a h eth to he e e h th ht  a a  ho  hoeoao eet h  etooe th  t a t     te  eeo tt o eoo  aa   e t   eo te ot  heoeeoe eaea  o toe   tet heeoe  oee eohaet   eaao e  ht o  aataotaeo   aoe t  e heee   e ohtte ea ht    hoaeoate ot teoo ho  ha  eooh te   e eet  he      h e  t  teet h tteat eo o  e e  eeo te t o ht aaonoeoe  e e o t ete   thoaootta e   eaeettto e  t      aa hteteohteoe te hethaae   h  h  t  tto   atee  eat  eot o a ae  heoe  tette  e t oe  t e  he   tohe t eeae too o  h ae ha eohaao tot  at    a    hte  t eoeet tohe\n"
     ]
    }
   ],
   "source": [
    "prompt='A'\n",
    "pick_top_chars=5\n",
    "num_chars=750\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #TF 1.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #sess.run(tf.initialize_all_variables())\n",
    "    saver.restore(sess, \"RNN_GRU_model_shakespeare\")\n",
    "    \n",
    "    state = None\n",
    "    current_char = vocab_to_idx[prompt]\n",
    "    chars = [current_char]\n",
    "    \n",
    "    for i in range(num_chars):\n",
    "            if state is not None:\n",
    "                preds, state = sess.run([predictions,final_state], feed_dict={x: [[current_char]], init_state: state})\n",
    "            else:\n",
    "                preds, state = sess.run([predictions,final_state], feed_dict={x: [[current_char]]})\n",
    "\n",
    "            if pick_top_chars is not None:\n",
    "                p = np.squeeze(preds)\n",
    "                p[np.argsort(p)[:-pick_top_chars]] = 0\n",
    "                p = p / np.sum(p)\n",
    "                current_char = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "            else:\n",
    "                current_char = np.random.choice(vocab_size, 1, p=np.squeeze(preds))[0]\n",
    "\n",
    "            chars.append(current_char)\n",
    "            \n",
    "chars = map(lambda x: idx_to_vocab[x], chars)\n",
    "print(\"\".join(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
